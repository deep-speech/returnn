#!crnn/rnn.py
# kate: syntax python;

import os
from Util import get_login_username
demo_name, _ = os.path.splitext(__file__)
print("Hello, experiment: %s" % demo_name)

# task
use_tensorflow = True
task = config.value("task", "train")

if task == "train":
    beam_size = 4
else:
    beam_size = 12

# data
num_inputs = 10
num_outputs = {"data": [num_inputs,1], "classes": [num_inputs,1]}
train = {"class": "CopyTaskDataset", "nsymbols": num_inputs, "num_seqs": 1000, "minlen": 1, "maxlen_epoch_factor": 20}
dev = {"class": "CopyTaskDataset", "nsymbols": num_inputs, "num_seqs": 50, "minlen": 1, "maxlen_epoch_factor": 20}

batch_size = 5000
max_seqs = 10
chunking = "0"

_isize = 5
l2 = 0.01

# network topology based on:
# https://github.com/rwth-i6/returnn-experiments/blob/master/2019-librispeech-system/attention/base2.bs18k.curric3.config
# adapted/simplified/extended for hard attention

EncKeyTotalDim = 20
EncValueTotalDim = 20
target = "classes"


network = {
"input": {"class": "linear", "activation": "tanh", "n_out": 20},

"encoder": {"class": "copy", "from": "input"},  # dim: EncValueTotalDim
"enc_ctx": {"class": "linear", "activation": None, "with_bias": True, "from": ["encoder"], "n_out": EncKeyTotalDim},
"enc_value": {"class": "copy", "from": "encoder"},  # (B, enc-T, D)

"output": {"class": "rec", "from": [], 'only_on_search': True, 'cheating': config.bool("cheating", False), "unit": {
    "s_transformed": {"class": "linear", "activation": None, "with_bias": False, "from": ["s"], "n_out": EncKeyTotalDim},
    "energy_in": {"class": "combine", "kind": "add", "from": ["base:enc_ctx", "s_transformed"], "n_out": EncKeyTotalDim},
    "energy_tanh": {"class": "activation", "activation": "tanh", "from": ["energy_in"]},
    "energy": {"class": "linear", "activation": None, "with_bias": False, "from": ["energy_tanh"], "n_out": 1},  # (B, enc-T, 1)
    "energy0": {"class": "squeeze", "axis": "f", "from": "energy"},  # (B, enc-T)
    "att_weights": {"class": "softmax_over_spatial", "from": "energy0", "start": "prev:t"},  # (B, enc-T)
    # ChoiceLayer works on the feature axis.
    "att_weights0": {"class": "reinterpret_data", "from": "att_weights", "set_axes": {"f": "t"}},

    "t": {
        "class": "choice", "from": "att_weights0", "target": None, "beam_size": beam_size,
        "length_normalization": False, "initial_output": 0},  # (B,)

    "att": {"class": "generic_attention", "weights": "att_weights", "base": "base:enc_value"},  # (B, V)

    "s": {"class": "rnn_cell", "unit": "LSTMBlock", "from": ["prev:target_embed", "prev:att"], "n_out": 20},
    "readout_in": {"class": "linear", "from": ["s", "prev:target_embed", "att"], "activation": None, "n_out": 50},
    "readout": {"class": "reduce_out", "mode": "max", "num_pieces": 2, "from": ["readout_in"]},
    "output_prob": {
        "class": "softmax", "from": ["readout"],
        "target": target, "loss": "ce", "loss_opts": {"label_smoothing": 0.1}},

    'output': {
        'class': 'choice', 'target': target, 'search': task != 'train',
        'beam_size': beam_size, 'cheating': config.bool("cheating", False), 'from': ["output_prob"],
        "initial_output": 0},
    "end": {"class": "compare", "from": ["output"], "value": 0},
    'target_embed': {'class': 'linear', 'activation': None, "with_bias": False, 'from': ['output'], "n_out": 20, "initial_output": 0},

}, "target": target, "max_seq_len": "max_len_from('base:encoder')"},

}

search_train_network_layers = ["output"]
debug_print_layer_output_template = True

adam = True
learning_rate = 0.01
gradient_noise = 0.3
gradient_clip = 2
# Note that this behavior here is slightly off because the cross-validation dev-set will get harder in each epoch.
use_last_best_model = {"modulo": 5, "filter_score": 1.5, "only_last_n": 5, "min_score_dist": 0.2}
model = "/tmp/%s/crnn/%s/model" % (get_login_username(), demo_name)  # https://github.com/tensorflow/tensorflow/issues/6537
num_epochs = 100
log_verbosity = 4
